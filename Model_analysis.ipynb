{"cells":[{"cell_type":"markdown","source":["\n","# üéØ Goal: Building an Early Warning System\n","\n","---\n","\n","## Objective: Early Warning System for financial crisis\n","\n","Financial markets are shaped by shifts in investor risk appetite, alternating between ‚Äúrisk-on‚Äù phases (when investors seek higher returns), and ‚Äúrisk-off‚Äù phases (when they retreat to safety). Identifying these transitions early can provide a critical edge in asset allocation and risk control. Therefore, the aim of this project is to develop an **Early Warning System **based on **anomaly detection**, treating risk-off regimes as abnormal deviations from typical market dynamics. Using engineered features derived from macro and market indicators, we apply different machine learning techniques to detect these shifts in real time. The goal is not to predict crises, but to recognize them as they begin‚Äîoffering a practical, data-driven approach to systemic risk monitoring.\n","\n","After developing such system, we will use it for developing our **trading strategy**.\n","\n"],"metadata":{"id":"J4RsTf6hKToE"}},{"cell_type":"markdown","metadata":{"id":"Fvw-xkfXqOqr"},"source":["## **PREPARING THE DATA:**\n","\n","We will experiment with several different models in order to find the one that works the best. For each model, we report a comprehensive set of evaluation metrics to assess its performance. At the end of the notebook, we summarize the results in a final table that compares all models across the key metrics.\n","\n","In this case we skip the data analysis and features selection / engineering part that we implement in another specific notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pqaoc4QqqMa_","outputId":"c3adbb54-80e1-4f06-f9c3-6b36c57c8765","executionInfo":{"status":"ok","timestamp":1748957315378,"user_tz":-120,"elapsed":70117,"user":{"displayName":"Luca Capoferri","userId":"10156989241841322848"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Import necessary libraries for data analysis and visualization\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy.stats import multivariate_normal\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","# Configure visualization settings\n","sns.set_theme(style=\"whitegrid\")\n","sns.set_context(\"notebook\", font_scale=1.5)\n","\n","# Mount Google Drive to access files\n","# Skip remounting if already mounted\n","try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","except:\n","    print(\"Drive already mounted or not in Colab environment.\")\n"]},{"cell_type":"markdown","metadata":{"id":"4Vt59qiCu-p9"},"source":["#### Real-World dataset\n","\n","As already explained we now upload a real-world dataset from Bloomberg, consisting of weekly observations.    \n","\n","<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BO4VrgL1qtgK"},"outputs":[],"source":["# Define the path to the dataset\n","file_path = '/content/drive/MyDrive/Fintech/BC4/Dataset4_EWS.xlsx'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"-FmrVMCO4RBE","outputId":"e5ed5331-b626-4de1-b19c-ebb55e941d29","executionInfo":{"status":"error","timestamp":1748957315795,"user_tz":-120,"elapsed":413,"user":{"displayName":"Luca Capoferri","userId":"10156989241841322848"}}},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/Fintech/BC4/Dataset4_EWS.xlsx'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-a6919fed02e7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load the data from the Excel file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# First sheet contains market data with dates and anomaly labels, second sheet contains metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdata_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Markets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmetadata_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Fintech/BC4/Dataset4_EWS.xlsx'"]}],"source":["# Set visualization style\n","sns.set_theme(style=\"whitegrid\")\n","plt.rcParams['figure.figsize'] = [10, 6]\n","\n","# Load the data from the Excel file\n","# First sheet contains market data with dates and anomaly labels, second sheet contains metadata\n","data_df = pd.read_excel(file_path, sheet_name='Markets')\n","metadata_df = pd.read_excel(file_path, sheet_name='Metadata')\n","\n","# Check the structure of the loaded data\n","print(\"Data columns:\", data_df.columns.tolist())\n","\n","# Extract date and anomaly label columns\n","date_col = 'Date' if 'Date' in data_df.columns else data_df.columns[0]\n","y_col = 'Y' if 'Y' in data_df.columns else None\n","\n","# Convert dates to datetime format\n","data_df[date_col] = pd.to_datetime(data_df[date_col], dayfirst=True)  # Date format is dd/mm/yy\n","\n","# Set date as index\n","data_df = data_df.set_index(date_col)\n","\n","# Extract features (all columns except Y if it exists)\n","if y_col:\n","    X_df = data_df.drop(y_col, axis=1)\n","    y = data_df[y_col].values\n","else:\n","    X_df = data_df\n","    y = None\n","\n","# Display basic information about the dataset\n","print(f\"Data shape: {X_df.shape}\")\n","print(f\"Total number of records: {len(X_df)}\")\n","print(f\"Time period: from {X_df.index.min().strftime('%m/%d/%Y')} to {X_df.index.max().strftime('%m/%d/%Y')}\")\n","print(f\"Frequency: {pd.infer_freq(X_df.index) or 'Weekly'}\")\n","print(f\"Number of variables: {X_df.shape[1]}\")\n","if y_col:\n","    print(f\"Number of anomalies: {np.sum(y == 1)} ({np.mean(y == 1)*100:.2f}%)\")\n","\n","# Create a more comprehensive metadata table with additional statistics\n","enhanced_metadata = []\n","\n","# Determine the correct column names for ticker and description\n","ticker_col = 'ticker' if 'ticker' in metadata_df.columns else metadata_df.columns[0]\n","desc_col = 'description' if 'description' in metadata_df.columns else metadata_df.columns[1] if len(metadata_df.columns) > 1 else ticker_col\n","\n","for ticker in X_df.columns:\n","    # Get metadata for this ticker if available\n","    meta_row = metadata_df[metadata_df[ticker_col] == ticker] if ticker in metadata_df[ticker_col].values else pd.DataFrame()\n","\n","    # Get description or use ticker if not found\n","    description = meta_row[desc_col].values[0] if not meta_row.empty and desc_col in meta_row.columns else ticker\n","\n","    # Calculate statistics for this series\n","    series = X_df[ticker]\n","\n","    enhanced_metadata.append({\n","        'Ticker': ticker,\n","        'Description': description,\n","        'Mean': series.mean(),\n","        'Std.Dev': series.std(),\n","        'Min': series.min(),\n","        'Max': series.max(),\n","        'Missing values': series.isna().sum(),\n","        'Missing (%)': f\"{series.isna().mean()*100:.2f}%\"\n","    })\n","\n","# Create enhanced metadata dataframe\n","enhanced_meta_df = pd.DataFrame(enhanced_metadata)\n","\n","# Display the enhanced metadata\n","print(\"\\nMetadata and statistics:\")\n","display(enhanced_meta_df)\n","\n","# Create a plot with anomalies as vertical bars and MXUS as a line\n","if y_col and 'MXUS' in X_df.columns:\n","    fig, ax = plt.subplots(figsize=(14, 8))\n","\n","    # Plot MXUS line\n","    ax.plot(X_df.index, X_df['MXUS'], color='darkred', linewidth=2.5, label='MSCI USA')\n","\n","    # Get the y-axis limits after plotting MXUS\n","    y_min, y_max = ax.get_ylim()\n","\n","    # For each anomaly point (Y=1), create a vertical span across the entire plot\n","    for i, (date, is_anomaly) in enumerate(zip(X_df.index, y)):\n","        if is_anomaly == 1:\n","            ax.axvspan(date, date + pd.Timedelta(days=7), alpha=0.3, color='navy', label='Risk-on/Risk-off' if i == 0 else \"\")\n","\n","    # Set labels and title\n","    ax.set_xlabel('Timeline')\n","    ax.set_ylabel('MSCI USA')\n","    ax.set_title('US Equities and risk-on/risk-off periods')\n","\n","    # Add legend\n","    handles, labels = ax.get_legend_handles_labels()\n","    by_label = dict(zip(labels, handles))\n","    ax.legend(by_label.values(), by_label.keys(), loc='best')\n","\n","    plt.tight_layout()\n","    plt.show()\n","else:\n","    print(\"Either 'Y' column or 'MXUS' column is missing in the dataset.\")\n"]},{"cell_type":"code","source":["##### STATIONARITY  #######\n","# We define a list of variables, divided by type. According to the type of feature, the timeseries are made stationary\n","\n","indices_currencies = [col for col in X_df.columns if col in [\n","    'XAUBGNL', 'BDIY', 'CRY', 'Cl1', 'DXY', 'EMUSTRUU', 'GBP', 'JPY', 'LF94TRUU',\n","    'LF98TRUU', 'LG30TRUU', 'LMBITR', 'LP01TREU', 'LUACTRUU', 'LUMSTRUU',\n","    'MXBR', 'MXCN', 'MXEU', 'MXIN', 'MXJP', 'MXRU', 'MXUS', 'VIX'\n","]]\n","\n","interest_rates = [col for col in X_df.columns if col in [\n","    'EONIA', 'GTDEM10Y', 'GTDEM2Y', 'GTDEM30Y', 'GTGBP20Y', 'GTGBP2Y', 'GTGBP30Y',\n","    'GTITL10YR', 'GTITL2YR', 'GTITL30YR', 'GTJPY10YR', 'GTJPY2YR', 'GTJPY30YR',\n","    'US0001M', 'USGG3M', 'USGG2YR', 'GT10', 'USGG30YR'\n","]]\n","\n","# Create a new dataframe for stationary data\n","stationary_df = pd.DataFrame(index=X_df.index[1:])\n","\n","# Apply log-differences to indices and currencies (always positive)\n","for col in indices_currencies:\n","    if col in X_df.columns:\n","        stationary_df[col] = np.diff(np.log(X_df[col]))\n","\n","# Apply first differences to interest rates (can be negative or very close to 0)\n","for col in interest_rates:\n","    if col in X_df.columns:\n","        stationary_df[col] = np.diff(X_df[col])\n","\n","# Keep Bloomberg Economic US Surprise Index as is (already stationary)\n","if 'ECSURPUS' in X_df.columns:\n","    stationary_df['ECSURPUS'] = X_df['ECSURPUS'].values[1:]\n","\n","# Adjust the response variable to match the new data length\n","if y is not None:\n","    y_stationary = y[1:]\n","else:\n","    y_stationary = None\n"],"metadata":{"id":"3GF4RwslOiaw"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zPhBuQlp8IH-"},"outputs":[],"source":["print(stationary_df.columns) # just to check if stationary colums are ok"]},{"cell_type":"markdown","source":["## **MODELS AND TUNING  WITH OPTUNA**\n","\n","Now we start to analyze all the models.\n","For each model we perform a tuning of some trials thanks to optuna library."],"metadata":{"id":"SpEpwn9iO3uU"}},{"cell_type":"code","source":["!pip install optuna"],"metadata":{"id":"pMrGDDFp1WRM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 1: shuffle and split data into training, validation, and test sets\n","from sklearn.utils import shuffle\n","from sklearn.preprocessing import StandardScaler\n","\n","# Set visualization style\n","sns.set_theme(style=\"whitegrid\")\n","plt.rcParams['figure.figsize'] = [12, 8]\n","new_stationary_df = pd.DataFrame(stationary_df[['Cl1', 'MXUS', 'MXEU']])\n","\n","# Convert to numpy arrays for easier manipulation\n","X = stationary_df.values\n","#X = new_stationary_df.values\n","y = y_stationary\n","\n","# Step 1: Creating training/cross-validation/test set with reshuffling\n","\n","# Reshuffle the data (this will break down autocorrelation)\n","X_shuffled, y_shuffled = shuffle(X, y, random_state=42)\n","\n","# Separate normal and anomalous examples\n","X_normal = X_shuffled[y_shuffled == 0]\n","X_anomaly = X_shuffled[y_shuffled == 1]\n","\n","# Calculate sizes for each set\n","n_normal = X_normal.shape[0]\n","n_anomaly = X_anomaly.shape[0]\n","\n","# Training set: 80% of normal examples\n","train_size = int(0.8 * n_normal)\n","X_train = X_normal[:train_size]\n","\n","# Cross-validation set: 10% of normal examples and 50% of anomalies\n","cv_normal_size = int(0.1 * n_normal)\n","cv_anomaly_size = int(0.5 * n_anomaly)\n","X_cv_normal = X_normal[train_size:train_size + cv_normal_size]\n","X_cv_anomaly = X_anomaly[:cv_anomaly_size]\n","X_cross_val = np.vstack((X_cv_normal, X_cv_anomaly))\n","y_cross_val = np.hstack((np.zeros(cv_normal_size), np.ones(cv_anomaly_size)))\n","\n","# Test set: 10% of normal examples and 50% of anomalies\n","X_test_normal = X_normal[train_size + cv_normal_size:]\n","X_test_anomaly = X_anomaly[cv_anomaly_size:]\n","X_test = np.vstack((X_test_normal, X_test_anomaly))\n","y_test = np.hstack((np.zeros(len(X_test_normal)), np.ones(len(X_test_anomaly))))\n","\n","# We'll standardize the data\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_cross_val = scaler.transform(X_cross_val)\n","X_test = scaler.transform(X_test)\n","\n","print(f\"Training set size: {X_train.shape[0]} (all normal)\")\n","print(f\"Cross-validation set size: {X_cross_val.shape[0]} ({cv_normal_size} normal, {cv_anomaly_size} anomalies)\")\n","print(f\"Test set size: {X_test.shape[0]} ({len(X_test_normal)} normal, {len(X_test_anomaly)} anomalies)\")\n"],"metadata":{"id":"oHIuIjsfPL9-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fx4wK1CFEHVj"},"source":["### 1) MVG-based anomaly detector with supervised threshold tuning\n","\n","We now apply a **Multivariate Gaussian anomaly detector**  to this real-world dataset of financial assets. This is a probabilistic model trained on normal data only, and will be our baseline model, the benchmark.\n","\n","We follow these steps:\n","\n","1. Fit the distribution of normal data.\n","2. Score all points using the estimated PDF.\n","3. Tune the threshold $\\varepsilon$ on a labeled validation set.\n","4. Evaluate on a held-out test set.\n"]},{"cell_type":"markdown","metadata":{"id":"vReWmAgBZ3s8"},"source":["#### Model fit and threshold tuning\n","\n","We fit the model on the training set, compute the PDF on the validation set, and search for the best threshold $\\varepsilon$ by maximizing the F1 score.\n"]},{"cell_type":"code","source":["# Step 2: Training the model (estimating parameters of multivariate Gaussian)\n","\n","# Calculate mean vector\n","mu = np.mean(X_train, axis=0)\n","\n","# Calculate covariance matrix\n","sigma = np.cov(X_train, rowvar=False)\n","\n","print(f\"Mean vector shape: {mu.shape}\")\n","print(f\"Covariance matrix shape: {sigma.shape}\")\n"],"metadata":{"id":"sQvswWmQP_ie"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cc8gA3FEbEBL"},"outputs":[],"source":["# Step 3: Fine tuning the hyperparameter, the threshold œµ\n","\n","# Function to calculate multivariate Gaussian PDF\n","def multivariate_gaussian_pdf(X, mu, sigma):\n","    \"\"\"Calculate the multivariate Gaussian probability density function\"\"\"\n","    n = mu.shape[0]\n","\n","    # Handle potential numerical issues with the covariance matrix\n","    # Add a small regularization term to ensure positive definiteness\n","    sigma_reg = sigma + np.eye(n) * 1e-8\n","\n","    # Calculate determinant and inverse\n","    try:\n","        det = np.linalg.det(sigma_reg)\n","        inv = np.linalg.inv(sigma_reg)\n","    except np.linalg.LinAlgError:\n","        # If still having issues, use pseudo-inverse\n","        print(\"Warning: Using pseudo-inverse for covariance matrix\")\n","        det = max(np.linalg.det(sigma_reg), 1e-10)\n","        inv = np.linalg.pinv(sigma_reg)\n","\n","    # Calculate PDF for each example\n","    p = np.zeros(X.shape[0])\n","    for i in range(X.shape[0]):\n","        x_mu = X[i] - mu\n","        p[i] = (1.0 / (np.power(2 * np.pi, n/2) * np.sqrt(det))) * \\\n","               np.exp(-0.5 * x_mu.dot(inv).dot(x_mu))\n","\n","    return p\n","\n","# Compute the probability density function for the cross-validation set\n","p_cv = multivariate_gaussian_pdf(X_cross_val, mu, sigma)\n","#just to try with pca data\n","#p_cv = multivariate_gaussian_pdf(x_cross_pca, mu_pca, sigma_pca)\n","# Find the range of epsilon values to search\n","min_epsilon = np.min(p_cv)\n","max_epsilon = np.max(p_cv)\n","step_size = (max_epsilon - min_epsilon) / 1000\n","\n","# Find the best epsilon using F1 score\n","best_epsilon = 0\n","best_f1 = 0\n","best_precision = 0\n","best_recall = 0\n","\n","epsilon_values = np.arange(min_epsilon, max_epsilon, step_size)\n","f1_scores = []\n","precisions = []\n","recalls = []\n","\n","for epsilon in epsilon_values:\n","    predictions = (p_cv < epsilon).astype(int)\n","\n","    # Calculate metrics\n","    precision = precision_score(y_cross_val, predictions, zero_division=0)\n","    recall = recall_score(y_cross_val, predictions, zero_division=0)\n","\n","    # Calculate F1 score\n","    if precision + recall > 0:  # Avoid division by zero\n","        f1 = 2 * precision * recall / (precision + recall)\n","    else:\n","        f1 = 0\n","\n","    f1_scores.append(f1)\n","    precisions.append(precision)\n","    recalls.append(recall)\n","\n","    if f1 > best_f1:\n","    #if recall > best_recall:\n","        best_f1 = f1\n","        best_epsilon = epsilon\n","        best_precision = precision\n","        best_recall = recall\n","\n","print(f\"Best F1 score on CV set: {best_f1:.4f}\")\n","print(f\"Best Epsilon: {best_epsilon:.8e}\")\n","print(f\"Corresponding Precision: {best_precision:.4f}\")\n","print(f\"Corresponding Recall: {best_recall:.4f}\")\n","\n","# Plot F1 score, precision, and recall vs epsilon\n","plt.figure(figsize=(14, 8))\n","plt.plot(epsilon_values, f1_scores, 'b-', label='F1 score')\n","plt.plot(epsilon_values, precisions, 'g-', label='Precision')\n","plt.plot(epsilon_values, recalls, 'r-', label='Recall')\n","plt.axvline(x=best_epsilon, color='k', linestyle='--', label=f'Best Epsilon: {best_epsilon:.2e}')\n","plt.xlabel('Epsilon')\n","plt.ylabel('Score')\n","plt.title('Metrics vs. Epsilon value')\n","plt.legend()\n","plt.xscale('log')  # Log scale for better visualization\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"FA18ox-EbbCf"},"source":["#### Model evaluation on test set\n","\n","We apply the selected threshold to the test set (unseen data) and evaluate the results using precision, recall, F1 score, and confusion matrix. We visualize the classification outcome using two selected features, to better understand false positives and false negatives in feature space.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WtLQb2JyEKkx"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix\n","\n","\n","# Step 4: Testing the model\n","\n","# Compute the probability density function for the test set\n","p_test = multivariate_gaussian_pdf(X_test, mu, sigma)\n","#p_test_pca = multivariate_gaussian_pdf(x_test_pca, mu_pca, sigma_pca)\n","# Make predictions using the best epsilon\n","predictions = (p_test < best_epsilon).astype(int)\n","\n","# Calculate metrics\n","MVG_precision = precision_score(y_test, predictions, zero_division=0)\n","MVG_recall = recall_score(y_test, predictions, zero_division=0)\n","MVG_f1 = f1_score(y_test, predictions, zero_division=0)\n","\n","print(\"\\nTest set performance:\")\n","print(f\"Precision: {MVG_precision:.4f}\")\n","print(f\"Recall: {MVG_recall:.4f}\")\n","print(f\"F1 Score: {MVG_f1:.4f}\")\n","\n","# Add autoencoder results to the comparison DataFrame\n","results_df = pd.DataFrame([\n","    (\"MVG\", MVG_precision, MVG_recall, MVG_f1)\n","], columns=['Model', 'Precision', 'Recall', 'F1 Score'])\n","\n","\n","# Create confusion matrix\n","cm = confusion_matrix(y_test, predictions)\n","tn, fp, fn, tp = cm.ravel()\n","\n","print(\"\\nConfusion Matrix:\")\n","print(f\"True Negatives: {tn}\")\n","print(f\"False Positives: {fp}\")\n","print(f\"False Negatives: {fn}\")\n","print(f\"True Positives: {tp}\")\n","\n","# Visualize confusion matrix\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","            xticklabels=['Normal', 'Anomaly'],\n","            yticklabels=['Normal', 'Anomaly'])\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.title('Confusion Matrix on Test set')\n","plt.tight_layout()\n","plt.show()\n","\n","\n","# Visualize examples of correctly and incorrectly classified points\n","# Select two features for visualization\n","if X_test.shape[1] >= 2:\n","    # Choose two features (columns) for visualization\n","    feature1_idx = 1  # First feature\n","    feature2_idx = 2  # Second feature\n","\n","    # Get feature names\n","    feature1_name = stationary_df.columns[feature1_idx]\n","    feature2_name = stationary_df.columns[feature2_idx]\n","\n","    plt.figure(figsize=(14, 10))\n","\n","    # True negatives (correctly classified normal points)\n","    plt.scatter(X_test[(y_test == 0) & (predictions == 0), feature1_idx],\n","                X_test[(y_test == 0) & (predictions == 0), feature2_idx],\n","                c='blue', marker='o', alpha=0.5, label='True Negative')\n","\n","    # False positives (normal points classified as anomalies)\n","    plt.scatter(X_test[(y_test == 0) & (predictions == 1), feature1_idx],\n","                X_test[(y_test == 0) & (predictions == 1), feature2_idx],\n","                c='green', marker='o', s=100, edgecolors='k', label='False Positive')\n","\n","    # False negatives (anomalies classified as normal)\n","    plt.scatter(X_test[(y_test == 1) & (predictions == 0), feature1_idx],\n","                X_test[(y_test == 1) & (predictions == 0), feature2_idx],\n","                c='purple', marker='o', s=100, edgecolors='k', label='False Negative')\n","\n","    # True positives (correctly classified anomalies)\n","    plt.scatter(X_test[(y_test == 1) & (predictions == 1), feature1_idx],\n","                X_test[(y_test == 1) & (predictions == 1), feature2_idx],\n","                c='red', marker='o', alpha=0.5, label='True Positive')\n","\n","    plt.xlabel(feature1_name)\n","    plt.ylabel(feature2_name)\n","    plt.title('Classification results on test set')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.tight_layout()\n","    plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"VvM36JxNknqL"},"source":["### Comparing Anomaly Detection Models\n","\n","We now explore several anomaly detection methods, from simpler models to deep learning. All models are tested on the **same financial dataset**, and evaluated using consistent metrics: **Precision, Recall, F1 Score, Confusion Matrix, ROC Curve**, and **PCA projections**.\n","\n","The categories of models are:\n","\n","- **MVG Baseline**: the baseline\n","- **Supervised models**: Random Forest, SVM.\n","- **Unsupervised models**: Isolation Forest, Local Outlier Factor, One-Class SVM, Gaussian Mixture.\n","- **Deep learning**: Autoencoder trained on normal data, scored via reconstruction error.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TOp894tPlPRw"},"outputs":[],"source":["# Step 0: Functions and utilities for comparison of supervised and unsupervised Anomaly Detection methods\n","\n","from sklearn.metrics import roc_curve, auc\n","from sklearn.ensemble import RandomForestClassifier, IsolationForest\n","from sklearn.svm import OneClassSVM, SVC\n","from sklearn.neighbors import LocalOutlierFactor\n","\n","# Set visualization style\n","sns.set_theme(style=\"whitegrid\")\n","plt.rcParams['figure.figsize'] = [12, 8]\n","\n","# We already have X_train, X_cross_val, y_cross_val, X_test, y_test from previous code cells\n","# We'll standardize the data for better performance with many algorithms\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_cv_scaled = scaler.transform(X_cross_val)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Function to evaluate and visualize model performance\n","def evaluate_model(y_true, y_pred, y_score, model_name, results_df=None, save_results=False):\n","    precision = precision_score(y_true, y_pred, zero_division=0)\n","    recall = recall_score(y_true, y_pred, zero_division=0)\n","    f1 = f1_score(y_true, y_pred, zero_division=0)\n","\n","    print(f\"\\n{model_name} Performance:\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","    print(f\"F1 Score: {f1:.4f}\")\n","\n","    if save_results:\n","        # Add results to the comparison DataFrame\n","      results_df.loc[len(results_df)] = [model_name, precision, recall, f1]\n","\n","    # Confusion Matrix\n","    cm = confusion_matrix(y_true, y_pred)\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                xticklabels=['Normal', 'Anomaly'],\n","                yticklabels=['Normal', 'Anomaly'])\n","    plt.xlabel('Predicted')\n","    plt.ylabel('Actual')\n","    plt.title(f'Confusion Matrix - {model_name}')\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # ROC Curve\n","    if y_score is not None:  # Some models don't provide probability scores\n","        fpr, tpr, _ = roc_curve(y_true, y_score)\n","        roc_auc = auc(fpr, tpr)\n","\n","        plt.figure(figsize=(8, 6))\n","        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n","        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","        plt.xlim([0.0, 1.0])\n","        plt.ylim([0.0, 1.05])\n","        plt.xlabel('False Positive Rate')\n","        plt.ylabel('True Positive Rate')\n","        plt.title(f'ROC Curve - {model_name}')\n","        plt.legend(loc=\"lower right\")\n","        plt.tight_layout()\n","        plt.show()\n","\n","    return precision, recall, f1\n","\n","# Function to visualize results in PCA space\n","def visualize_pca(X, y_true, y_pred, model_name):\n","    # Apply PCA\n","    pca = PCA(n_components=2)\n","    X_pca = pca.fit_transform(X)\n","\n","    # Create a DataFrame for plotting\n","    pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n","    pca_df['Actual'] = y_true\n","    pca_df['Predicted'] = y_pred\n","\n","    # Create classification categories\n","    pca_df['Category'] = 'Unknown'\n","    pca_df.loc[(y_true == 0) & (y_pred == 0), 'Category'] = 'True Negative'\n","    pca_df.loc[(y_true == 0) & (y_pred == 1), 'Category'] = 'False Positive'\n","    pca_df.loc[(y_true == 1) & (y_pred == 0), 'Category'] = 'False Negative'\n","    pca_df.loc[(y_true == 1) & (y_pred == 1), 'Category'] = 'True Positive'\n","\n","    # Calculate explained variance\n","    explained_variance = pca.explained_variance_ratio_\n","    total_variance = sum(explained_variance)\n","\n","    # Plot\n","    plt.figure(figsize=(12, 8))\n","\n","    # Define colors and sizes\n","    colors = {'True Negative': 'gray', 'True Positive': 'black',\n","              'False Positive': 'red', 'False Negative': 'blue'}\n","    alphas = {'True Negative': 0.3, 'True Positive': 0.5,\n","              'False Positive': 0.8, 'False Negative': 0.8}\n","    sizes = {'True Negative': 30, 'True Positive': 40,\n","             'False Positive': 80, 'False Negative': 80}\n","\n","    # Plot each category\n","    for category, group in pca_df.groupby('Category'):\n","        plt.scatter(group['PC1'], group['PC2'],\n","                    color=colors[category],\n","                    alpha=alphas[category],\n","                    s=sizes[category],\n","                    label=f\"{category} ({len(group)})\")\n","\n","    plt.title(f'PCA projection - {model_name}\\nExplained variance: {total_variance:.2%}', fontsize=16)\n","    plt.xlabel(f'PC1 ({explained_variance[0]:.2%})', fontsize=14)\n","    plt.ylabel(f'PC2 ({explained_variance[1]:.2%})', fontsize=14)\n","    plt.grid(True, alpha=0.3)\n","    plt.legend(fontsize=12)\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Store results for comparison\n","results = []\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Oniug0OPmb6b"},"source":["### 2) Supervised models\n","\n","These models are trained using the anomaly labels we have, thus they explicitly learn to separate normal and anomalous cases based on known examples. So, the training set includes both normal and anomalous data, combined from the original training and validation sets.\n","\n","In particular, we use:\n","\n","- **Random Forest Classifier**.\n","- **Support Vector Machine (SVM)**.\n","\n"]},{"cell_type":"markdown","source":["####2.1) Random Forest Classifier"],"metadata":{"id":"6AmNIYMzSJjR"}},{"cell_type":"code","source":["print(\"=\" * 50)\n","print(\"SUPERVISED ANOMALY DETECTION METHODS\")\n","print(\"=\" * 50)\n","\n","# Random Forest Classifier\n","print(\"\\nTraining Random Forest classifier...\")\n","# For supervised methods, we need to combine training and cross-validation sets in order to have some anomalies to train with\n","X_train_cv = np.vstack((X_train_scaled, X_cv_scaled))\n","y_train_cv = np.hstack((np.zeros(len(X_train_scaled)), y_cross_val))\n","\n","rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n","rf_model.fit(X_train_cv, y_train_cv)  # Train on combined data\n","\n","# Predict on test set\n","rf_pred = rf_model.predict(X_test_scaled)\n","rf_score = rf_model.predict_proba(X_test_scaled)[:, 1]  # Probability of class 1 (anomaly)\n","\n","# Evaluate\n","rf_metrics = evaluate_model(y_test, rf_pred, rf_score, \"Random Forest (Supervised)\",results_df, save_results=True)\n","results.append((\"Random Forest (Supervised)\", *rf_metrics))"],"metadata":{"id":"L58GuwuuR-xv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2.2 ) Support Vector Machine"],"metadata":{"id":"uVUWp5TDSUld"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-d4zDrpCn6co"},"outputs":[],"source":["print(\"\\nTraining Support Vector Machine...\")\n","svm_model = SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=42)\n","svm_model.fit(X_train_cv, y_train_cv)\n","\n","# Predict on test set\n","svm_pred = svm_model.predict(X_test_scaled)\n","svm_score = svm_model.predict_proba(X_test_scaled)[:, 1]\n","\n","# Evaluate\n","svm_metrics = evaluate_model(y_test, svm_pred, svm_score, \"SVM (Supervised)\", results_df, save_results=True)\n","results.append((\"SVM (Supervised)\", *svm_metrics))\n"]},{"cell_type":"markdown","metadata":{"id":"LBuUUOX2odlV"},"source":["### 3) Unsupervised Models\n","\n","We now test a series of unsupervised models ‚Äî these do **not use any labels during training** and rely solely on data structure. So we don't use our response vector Y during the training.\n","\n","The models we consider are:\n","\n","- **[Isolation Forest](https://en.wikipedia.org/wiki/Isolation_forest)**.\n","- **[One-Class SVM](https://en.wikipedia.org/wiki/One-class_classification)**.\n","- **[Local Outlier Factor (LOF)](https://en.wikipedia.org/wiki/Local_outlier_factor)**.\n","- **[Gaussian Mixture Model (GMM)](https://en.wikipedia.org/wiki/Mixture_model)**.\n","\n","All of them require an estimate of the expected **contamination** (anomaly rate), which we derive from the validation set for consistency. To avoid degenerate behavior, we cap the contamination at 50%.\n","\n","Here we use the original data split (train\\cv\\test).\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1fC3wcsWzgfU"},"outputs":[],"source":["print(\"\\n\" + \"=\" * 50)\n","print(\"UNSUPERVISED ANOMALY DETECTION METHODS\")\n","print(\"=\" * 50)\n","\n","# Calculate contamination from cross-validation set\n","raw_contamination = np.mean(y_cross_val)\n","print(f\"\\nEstimated contamination from cross-validation set: {raw_contamination:.4f}\")\n","\n","# Cap contamination for algorithms that have limits\n","contamination = min(raw_contamination, 0.5)\n","print(f\"Using capped contamination value: {contamination:.4f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"FYYhOlqbzlJo"},"source":["#### Isolation Forest\n","\n","The Isolation Forest algorithm isolates anomalies by recursively partitioning the data using random splits.\n","\n","It is efficient, scalable, and works well in high dimensions.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7WXyyM0N0fyl"},"outputs":[],"source":["# 3.1 Isolation Forest\n","print(\"\\nTraining Isolation Forest...\")\n","iso_model = IsolationForest(contamination=contamination, random_state=42)\n","iso_model.fit(X_train_scaled)\n","\n","# Predict on test set (convert from 1/-1 to 0/1, where 1 means anomaly)\n","iso_pred = (iso_model.predict(X_test_scaled) == -1).astype(int)\n","iso_score = -iso_model.score_samples(X_test_scaled)  # Negative of the anomaly score\n","\n","# Evaluate\n","iso_metrics = evaluate_model(y_test, iso_pred, iso_score, \"Isolation Forest (Unsupervised)\", results_df, save_results=True)\n","results.append((\"Isolation Forest (Unsupervised)\", *iso_metrics))\n"]},{"cell_type":"markdown","metadata":{"id":"wRGJLsQP0pR4"},"source":["#### One-Class SVM\n","\n","The One-Class SVM algorithm learns a soft boundary that encloses the majority of the data, treating anything outside this boundary as an anomaly.\n","\n","It is sensitive to the **nu** parameter, which controls the upper bound on the anomaly fraction and the lower bound on the number of support vectors.\n","\n","We set `nu` based on the estimated contamination rate, capped to avoid instability.\n","\n","<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ptUW7Ahl1caq"},"outputs":[],"source":["# 3.2 One-Class SVM\n","print(\"\\nTraining One-Class SVM...\")\n","# OneClassSVM's nu parameter is the upper bound on the fraction of training errors\n","# and the lower bound of the fraction of support vectors, so it should be in (0, 1)\n","nu = min(max(0.01, contamination), 0.99)\n","print(f\"Using nu value for OneClassSVM: {nu:.4f}\")\n","\n","ocsvm_model = OneClassSVM(gamma='auto', nu=nu)\n","ocsvm_model.fit(X_train_scaled)\n","\n","# Predict on test set\n","ocsvm_pred = (ocsvm_model.predict(X_test_scaled) == -1).astype(int)\n","ocsvm_score = -ocsvm_model.decision_function(X_test_scaled)  # Negative of the decision function\n","\n","# Evaluate\n","ocsvm_metrics = evaluate_model(y_test, ocsvm_pred, ocsvm_score, \"One-Class SVM (Unsupervised)\", results_df, save_results=True)\n","results.append((\"One-Class SVM (Unsupervised)\", *ocsvm_metrics))\n"]},{"cell_type":"markdown","metadata":{"id":"D8RnXlWc1hnX"},"source":["#### Local Outlier Factor (LOF)\n","\n","LOF detects anomalies by comparing the **local density** of a point to that of its neighbors.\n","\n","Points that reside in **sparser regions** relative to their surroundings are flagged as outliers.\n","\n","It is sensitive to the number of neighbors and to the contamination parameter.  \n","We use `novelty=True` to enable scoring on new, unseen test data.\n","\n","<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iUPJbbMO10Tn"},"outputs":[],"source":["# 3.3 Local Outlier Factor\n","print(\"\\nTraining Local Outlier Factor...\")\n","# LOF also has a contamination parameter with the same constraints as Isolation Forest\n","lof_model = LocalOutlierFactor(n_neighbors=48, contamination=contamination, novelty=True)\n","lof_model.fit(X_train_scaled)\n","\n","# Predict on test set\n","lof_pred = (lof_model.predict(X_test_scaled) == -1).astype(int)\n","lof_score = -lof_model.decision_function(X_test_scaled)  # Negative of the decision function\n","\n","# Evaluate\n","lof_metrics = evaluate_model(y_test, lof_pred, lof_score, \"Local Outlier Factor (Unsupervised)\", results_df, save_results=True)\n","results.append((\"Local Outlier Factor (Unsupervised)\", *lof_metrics))\n"]},{"cell_type":"markdown","source":["**OPTUNA**\n","\n","We use now an automated hyperparameter tuning to improve the performance of the *LocalOutlierFactor* model, which was the one with the higher metrics. We specifically utilize the **optuna library** to search for the optimal value for the n_neighbors parameter, which is a key setting for this algorithm.\n","\n","We find the optimal number of *n_neighbors* to be *48*, which increases the F1 score from  0.7444  to  **0.7576**"],"metadata":{"id":"hdcZr-d0TBjU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5sfaxUDxpfnM"},"outputs":[],"source":["# optuna for LOF:\n","import optuna\n","def objective(trial):\n","    n_neighbors = trial.suggest_int(\"n_neighbors\", 5, 50)\n","    lof_model = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination, novelty=True)\n","    lof_model.fit(X_train_scaled)\n","    lof_pred = (lof_model.predict(X_test_scaled) == -1).astype(int)\n","    lof_score = -lof_model.decision_function(X_test_scaled)\n","    lof_metrics = evaluate_model(y_test, lof_pred, lof_score, \"Local Outlier Factor (Unsupervised)\")\n","    return lof_metrics[1]  # Return the F1 score\n","study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=10)\n","best_params = study.best_params\n","print(\"Best hyperparameters:\", best_params)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M9802z-4rZdv"},"outputs":[],"source":["best_params = study.best_params\n","print(\"Best hyperparameters:\", best_params)\n","print(f\"f1_score: {lof_metrics[2]}\")"]},{"cell_type":"markdown","metadata":{"id":"HoGDnmoO15Qu"},"source":["#### Gaussian Mixture Model (GMM)\n","\n","We now use a **2-component Gaussian Mixture Model** to estimate  the log-likelihood of each point under a flexible, **multimodal distribution**.\n","\n","This approach can capture more complex patterns in the \"normal\" data,\n","compared to a single Gaussian (baseline model) which imposes an ellipsoidal structure. Points with **low likelihood** are flagged as anomalies.\n","\n","We use the contamination rate from the validation set to define the decision threshold.\n","\n","<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kP_EWF0Uqu7P","collapsed":true},"outputs":[],"source":["# 3.4 Gaussian Mixture Model - 2 components\n","from sklearn.mixture import GaussianMixture\n","\n","print(\"\\nTraining Gaussian Mixture Model...\")\n","gmm_model = GaussianMixture(n_components=2, covariance_type='full', random_state=42)\n","gmm_model.fit(X_train_scaled)\n","\n","# Predict on test set\n","gmm_score = -gmm_model.score_samples(X_test_scaled)  # Negative log-likelihood as anomaly score\n","# For GMM we can use the raw contamination as it doesn't have the same constraints\n","threshold = np.percentile(gmm_score, 100 * (1 - raw_contamination))\n","gmm_pred = (gmm_score > threshold).astype(int)\n","\n","# Evaluate\n","gmm_metrics = evaluate_model(y_test, gmm_pred, gmm_score, \"Gaussian Mixture Model (Unsupervised)\", results_df, save_results=True)\n","results.append((\"Gaussian Mixture Model (Unsupervised)\", *gmm_metrics))\n"]},{"cell_type":"markdown","source":["###  4) Autoencoder & GAN for Anomaly Detection\n","\n","An **Autoencoder** is a neural network that learns to compress and reconstruct data:\n","\n","- **Encoder**: maps input to a lower-dimensional latent space.\n","- **Decoder**: reconstructs the original input from this compressed representation.\n","\n","This architecture captures the structure of normal data, making it useful for anomaly detection.\n","\n","In the following cells, we extend the Autoencoder into a **GAN-based model**. A **Generative Adversarial Network (GAN)** consists of:\n","\n","- A **Generator** (here, the decoder from our autoencoder) that tries to produce realistic data.\n","- A **Discriminator** that learns to distinguish real data from generated data.\n","\n","We first pretrained the Autoencoder briefly, then reuse its decoder as the Generator. We implement the Discriminator from scratch and train the GAN accordingly.\n","\n","While not our best-performing method, we believe this approach has strong potential with further tuning.\n"],"metadata":{"id":"ydTJzmzu2hi8"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"n6gQn-cRX1v9"},"outputs":[],"source":["# 4.  Deep Learning methods: Autoencoder\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","# Set random seeds for reproducibility\n","np.random.seed(42)\n","torch.manual_seed(42)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(42)\n","\n","# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#device = torch.device(\"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Convert data to PyTorch tensors\n","X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n","X_cv_tensor = torch.FloatTensor(X_cv_scaled).to(device)\n","X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n","\n","# Create DataLoaders\n","batch_size = 64\n","train_dataset = TensorDataset(X_train_tensor, X_train_tensor)  # Input = Output for autoencoder\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","# Define the Autoencoder architecture\n","class Autoencoder(nn.Module):\n","    def __init__(self, input_dim, encoding_dim=16):\n","        super(Autoencoder, self).__init__()\n","\n","        # Calculate layer sizes for a gradually decreasing architecture\n","        layer1_size = input_dim // 2\n","        layer2_size = layer1_size // 2\n","\n","        # Ensure minimum size\n","        layer1_size = max(layer1_size, encoding_dim * 2)\n","        layer2_size = max(layer2_size, encoding_dim)\n","\n","        # Encoder\n","        self.encoder = nn.Sequential(\n","            nn.Linear(input_dim, layer1_size),\n","            nn.BatchNorm1d(layer1_size),\n","            nn.ReLU(True),\n","            nn.Dropout(0.2),\n","            nn.Linear(layer1_size, layer2_size),\n","            nn.BatchNorm1d(layer2_size),\n","            nn.ReLU(True),\n","            nn.Dropout(0.2),\n","            nn.Linear(layer2_size, encoding_dim),\n","            nn.BatchNorm1d(encoding_dim),\n","            nn.ReLU(True)\n","        )\n","\n","        # Decoder\n","        self.decoder = nn.Sequential(\n","            nn.Linear(encoding_dim, layer2_size),\n","            nn.BatchNorm1d(layer2_size),\n","            nn.ReLU(True),\n","            nn.Dropout(0.2),\n","            nn.Linear(layer2_size, layer1_size),\n","            nn.BatchNorm1d(layer1_size),\n","            nn.ReLU(True),\n","            nn.Dropout(0.2),\n","            nn.Linear(layer1_size, input_dim),\n","            nn.Tanh()  # Output activation to match standardized data range\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n","\n","    def encode(self, x):\n","        return self.encoder(x)\n","\n","# Get input dimension from data\n","input_dim = X_train_scaled.shape[1]\n","print(f\"Input dimension: {input_dim}\")\n","\n","# Create the autoencoder model\n","encoding_dim = min(16, input_dim // 4)  # Adjust encoding dimension based on input size\n","model = Autoencoder(input_dim, encoding_dim).to(device)\n","print(f\"Encoding dimension: {encoding_dim}\")\n","print(model)\n","\n","# Define loss function and optimizer\n","#criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n","\n","# Training function\n","def train_autoencoder(model, train_loader, num_epochs=100, patience=10):\n","    criterion = nn.MSELoss()\n","    # For early stopping\n","    best_loss = float('inf')\n","    no_improve_epochs = 0\n","\n","    # For plotting\n","    train_losses = []\n","\n","    print(\"Training autoencoder...\")\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","\n","        for data, _ in train_loader:\n","            # Zero the gradients\n","            optimizer.zero_grad()\n","\n","            # Forward pass\n","            outputs = model(data)\n","            loss = criterion(outputs, data)\n","\n","            # Backward pass and optimize\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * data.size(0)\n","\n","        # Calculate epoch loss\n","        epoch_loss = running_loss / len(train_loader.dataset)\n","        train_losses.append(epoch_loss)\n","\n","        # Print progress\n","        if (epoch + 1) % 10 == 0:\n","            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.6f}')\n","\n","        # Early stopping\n","        if epoch_loss < best_loss:\n","            best_loss = epoch_loss\n","            no_improve_epochs = 0\n","            # Save the best model\n","            torch.save(model.state_dict(), 'best_autoencoder.pth')\n","        else:\n","            no_improve_epochs += 1\n","            if no_improve_epochs >= patience:\n","                print(f'Early stopping at epoch {epoch+1}')\n","                break\n","\n","    # Load the best model\n","    model.load_state_dict(torch.load('best_autoencoder.pth'))\n","\n","    # Plot training loss\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(train_losses)\n","    plt.title('Autoencoder Training Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('MSE Loss')\n","    plt.grid(True)\n","    plt.show()\n","\n","    return model\n"]},{"cell_type":"markdown","metadata":{"id":"YiL8N1wIMU5A"},"source":["###  Training with Early Stopping\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qy_hs0RR0nxZ"},"outputs":[],"source":["print(input_dim, encoding_dim) #just to check dimension"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"MYmhbS-JX-B8"},"outputs":[],"source":["model = train_autoencoder(model, train_loader, num_epochs=104, patience=32)\n","generator = model.decoder\n","class Discriminator(nn.Module):\n","    def __init__(self, input_dim, dropout=0.3):\n","        super(Discriminator, self).__init__()\n","        self.main = nn.Sequential(\n","            nn.Linear(input_dim, 256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Dropout(dropout),\n","            nn.Linear(256, 128),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Dropout(dropout),\n","            nn.Linear(128, 1),\n","            nn.Sigmoid()  # Output probability (real or fake)\n","        )\n","    def forward(self, x):\n","            return self.main(x)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aYzKVj00YK6h"},"outputs":[],"source":["discriminator = Discriminator(input_dim=input_dim, dropout=0.3).to(device)\n","optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002)\n","optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002)\n","#criterion = nn.BCELoss()\n","num_epochs = 100\n","latent_dim = encoding_dim\n","#ok now we have to train the discriminator:\n","def training_discriminator(num_epochs=100):\n","  criterion = nn.BCELoss()\n","  for epoch in range(num_epochs):\n","      for real_samples, _ in train_loader:\n","          real_samples = real_samples.view(real_samples.size(0), -1).to(device)\n","          batch_size = real_samples.size(0)\n","\n","          # === Train Discriminator ===\n","          real_labels = torch.ones(batch_size, 1).to(device)\n","          fake_labels = torch.zeros(batch_size, 1).to(device)\n","\n","          # Output del discriminator su dati reali\n","          outputs_real = discriminator(real_samples)\n","          loss_real = criterion(outputs_real, real_labels)\n","\n","          # Genera dati fake usando il decoder (generator)\n","          noise = torch.randn(batch_size, latent_dim).to(device)\n","          fake_samples = generator(noise)\n","\n","          # Output del discriminator su dati fake\n","          outputs_fake = discriminator(fake_samples.detach())\n","          loss_fake = criterion(outputs_fake, fake_labels)\n","\n","          # Backpropagation totale discriminator\n","          loss_D = loss_real + loss_fake\n","          optimizer_D.zero_grad()\n","          loss_D.backward()\n","          optimizer_D.step()\n","\n","          # === Train Generator ===\n","          outputs = discriminator(fake_samples)\n","          loss_G = criterion(outputs, real_labels)  # Vuole ingannare il discriminator\n","          optimizer_G.zero_grad()\n","          loss_G.backward()\n","          optimizer_G.step()\n","  return discriminator, generator\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U_iTCJvk7iXF"},"outputs":[],"source":["discriminator, generator = training_discriminator(num_epochs=100)\n","model.decoder = generator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l3RoWNH5YQVN"},"outputs":[],"source":["# Function to compute reconstruction error\n","def compute_reconstruction_error(model, data_tensor):\n","    model.eval()\n","    with torch.no_grad():\n","        reconstructions = model(data_tensor)\n","        # Compute MSE for each sample\n","        mse = ((reconstructions - data_tensor) ** 2).mean(dim=1).cpu().numpy()\n","    return mse\n","\n","# Compute reconstruction errors\n","train_errors = compute_reconstruction_error(model, X_train_tensor)\n","cv_errors = compute_reconstruction_error(model, X_cv_tensor)\n","print(f\"cv_errors= {cv_errors}\")\n","test_errors = compute_reconstruction_error(model, X_test_tensor)\n","\n","# Determine threshold for anomaly detection using cross-validation set\n","# We'll use the contamination rate from the cross-validation set\n","contamination = np.mean(y_cross_val)\n","print(f\"Contamination rate from cross-validation set: {contamination:.4f}\")\n","\n","# Find the threshold that best separates normal and anomalous examples in the CV set\n","cv_errors_normal = cv_errors[y_cross_val == 0]\n","cv_errors_anomaly = cv_errors[y_cross_val == 1]\n","\n","# Plot histogram of reconstruction errors by class\n","plt.figure(figsize=(12, 6))\n","plt.hist(cv_errors_normal, bins=50, alpha=0.5, label='Normal', color='blue')\n","plt.hist(cv_errors_anomaly, bins=50, alpha=0.5, label='Anomaly', color='red')\n","plt.axvline(x=np.percentile(cv_errors, 100 * (1 - contamination)),\n","            color='green', linestyle='dashed', linewidth=2,\n","            label=f'Threshold at {100 * (1 - contamination):.1f}th percentile')\n","plt.title('Reconstruction error distribution by class (Cross-Validation Set)')\n","plt.xlabel('Reconstruction error (MSE)')\n","plt.ylabel('Count')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","plt.show()\n","\n","# Set threshold at the percentile corresponding to the contamination rate\n","threshold = np.percentile(cv_errors, 100 * (1 - contamination))\n","print(f\"Threshold for anomaly detection: {threshold:.6f}\")\n","\n","# Make predictions on test set\n","y_pred = (test_errors > threshold).astype(int)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"s3sKgMKWQXiT"},"source":["### Evaluation metrics\n","\n"," In addition to the usual metrics, we show the plot with the **reconstruction error** for each point in the test set.\n","\n","- Each dot is a data point.\n","- Color indicates the true label:  \n","  - üîµ Blue for **normal**  \n","  - üî¥ Red for **anomalies**\n","- The green dashed line is the **threshold** used to classify a point as anomalous.\n","\n","What does this chart tell us?\n","\n","- Samples **above the threshold** are predicted as anomalies\n","- Samples **below** are classified as normal\n","- We can directly spot:\n","  - **False positives**: blue dots above the line\n","  - **False negatives**: red dots below the line\n","\n","This is a simple but effective diagnostic tool to evaluate the classifier's behavior **based on its internal logic** (reconstruction error).  \n","It's also a sanity check: are anomalies really harder to reconstruct?\n","\n","<br>\n","\n","---\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"iLmiPnSgP9JE"},"outputs":[],"source":["# Evaluate the autoencoder model\n","def evaluate_autoencoder(y_true, y_pred, reconstruction_errors, threshold):\n","    precision = precision_score(y_true, y_pred, zero_division=0)\n","    recall = recall_score(y_true, y_pred, zero_division=0)\n","    f1 = f1_score(y_true, y_pred, zero_division=0)\n","\n","    print(\"\\nAutoencoder performance:\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","    print(f\"F1 Score: {f1:.4f}\")\n","\n","    # Confusion Matrix\n","    cm = confusion_matrix(y_true, y_pred)\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                xticklabels=['Normal', 'Anomaly'],\n","                yticklabels=['Normal', 'Anomaly'])\n","    plt.xlabel('Predicted')\n","    plt.ylabel('Actual')\n","    plt.title('Confusion Matrix - Autoencoder')\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # ROC Curve\n","    fpr, tpr, _ = roc_curve(y_true, reconstruction_errors)\n","    roc_auc = auc(fpr, tpr)\n","\n","    plt.figure(figsize=(8, 6))\n","    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n","    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('ROC curve - Autoencoder')\n","    plt.legend(loc=\"lower right\")\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # Visualize reconstruction error distribution\n","    plt.figure(figsize=(10, 6))\n","    plt.scatter(range(len(reconstruction_errors)),\n","                reconstruction_errors,\n","                c=['blue' if label == 0 else 'red' for label in y_true],\n","                alpha=0.5)\n","    plt.axhline(y=threshold, color='green', linestyle='--', label=f'Threshold: {threshold:.4f}')\n","    plt.title('Reconstruction error for test set')\n","    plt.xlabel('Sample index')\n","    plt.ylabel('Reconstruction error')\n","    plt.legend()\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return precision, recall, f1, roc_auc\n","\n","\n","# Evaluate autoencoder performance\n","ae_metrics = evaluate_autoencoder(y_test, y_pred, test_errors, threshold)\n","results_df.loc[len(results_df)] = [\"GAN\", ae_metrics[0], ae_metrics[1], ae_metrics[2]]\n","\n","# Print the results table\n","print(\"\\nPerformance comparison:\")\n","print(results_df.sort_values('F1 Score', ascending=False).to_string(index=False))\n","\n","print(\"\\nMemento: Baseline model performance:\")\n","print(f\"Precision: {MVG_precision:.4f}\")\n","print(f\"Recall: {MVG_recall:.4f}\")\n","print(f\"F1 Score: {MVG_f1:.4f}\")\n"]},{"cell_type":"code","source":["train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"],"metadata":{"id":"Dgc6q1YSo2HM"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"IXzs7k84Vs4w"},"outputs":[],"source":["#using optuna for GAN\n","import optuna\n","def objective(trial):\n","      epochs = trial.suggest_int(\"epochs\", 10, 100)\n","      patience = trial.suggest_int(\"patience\", 5, 20)\n","      dropout= trial.suggest_float(\"dropout\", 0.2, 0.5)\n","      encoding_dim= trial.suggest_int(\"encoding_dim\", 10, 100)\n","      latent_dim=encoding_dim\n","      model = Autoencoder(input_dim, latent_dim)\n","      model = train_autoencoder(model,train_loader, epochs, patience)\n","      generator = model.decoder\n","      model = Discriminator(input_dim=input_dim, dropout=dropout)\n","      optimizer_D = torch.optim.Adam(model.parameters(), lr=0.0002)\n","      optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002)\n","      discriminator, generator = training_discriminator(num_epochs=num_epochs)\n","      model.decoder = generator\n","      cv_errors = compute_reconstruction_error(model, X_cv_tensor)\n","      threshold = np.percentile(cv_errors, 100 * (1 - contamination))\n","      print(f\"Threshold for anomaly detection: {threshold:.6f}\")\n","      y_pred = (cv_errors > threshold).astype(int)\n","      metrics = evaluate_autoencoder(y_cross_val, y_pred, cv_errors, threshold)\n","      return metrics[2]\n","\n","study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=10)\n","best_parameters = study.best_params\n","best_objective = study.best_value\n","print(\"Best trial:\")\n","print(\"  Value: \", best_objective)\n","print(\"  Params: \")\n","for key, value in best_parameters.items():\n","    print(\"    {}: {}\".format(key, value))"]},{"cell_type":"markdown","source":["### 5) XGBOOST\n","We implement now an **XGBoost (Extreme Gradient Boosting)** model for anomaly detection. Since XGBoost is a supervised learning algorithm, it requires labeled data for training. So we fix a little the dataset.\n","\n","\n","\n"],"metadata":{"id":"XjdRW4srSv-i"}},{"cell_type":"code","source":["!pip install xgboost"],"metadata":{"id":"wxLizi98S1Pi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 1: shuffle and split data into training, validation, and test sets\n","from sklearn.utils import shuffle\n","from sklearn.preprocessing import StandardScaler\n","\n","# Set visualization style\n","sns.set_theme(style=\"whitegrid\")\n","plt.rcParams['figure.figsize'] = [12, 8]\n","\n","\n","# Convert to numpy arrays for easier manipulation\n","X = stationary_df.values\n","#X = new_stationary_df.values\n","y = y_stationary\n","\n","# Step 1: Creating training/cross-validation/test set with reshuffling\n","\n","# Reshuffle the data (this will break down autocorrelation)\n","X_shuffled, y_shuffled = shuffle(X, y, random_state=42)\n","\n","# Separate normal and anomalous examples\n","X_normal = X_shuffled[y_shuffled == 0]\n","X_anomaly = X_shuffled[y_shuffled == 1]\n","\n","# Calculate sizes for each set\n","n_normal = X_normal.shape[0]\n","n_anomaly = X_anomaly.shape[0]\n","\n","# Training set: 80% of normal examples\n","train_normal_size = int(0.85* n_normal)\n","train_anomaly_size = int(0.9 * n_anomaly)\n","X_train_normal = X_normal[:train_normal_size]\n","X_train_anomaly = X_anomaly[:train_anomaly_size]\n","X_train = np.vstack((X_train_normal, X_train_anomaly))\n","y_train = np.hstack((np.zeros(train_normal_size), np.ones(train_anomaly_size)))\n","\n","\n","# Cross-validation set: 10% of normal examples and 50% of anomalies\n","cv_normal_size = int(0 * n_normal)\n","cv_anomaly_size = int(0 * n_anomaly)\n","#X_cv_normal = X_normal[train_size:train_size + cv_normal_size]\n","#X_cv_anomaly = X_anomaly[:cv_anomaly_size]\n","#X_cross_val = np.vstack((X_cv_normal, X_cv_anomaly))\n","#y_cross_val = np.hstack((np.zeros(cv_normal_size), np.ones(cv_anomaly_size)))\n","\n","# Test set: 10% of normal examples and 50% of anomalies\n","X_test_normal = X_normal[train_normal_size + cv_normal_size:]\n","X_test_anomaly = X_anomaly[cv_anomaly_size:]\n","X_test = np.vstack((X_test_normal, X_test_anomaly))\n","y_test = np.hstack((np.zeros(len(X_test_normal)), np.ones(len(X_test_anomaly))))\n","\n","# We'll standardize the data\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_cross_val = scaler.transform(X_cross_val)\n","X_test = scaler.transform(X_test)\n","\n","print(f\"Training set size: {X_train.shape[0]} {y_train.shape[0]}(both normal or anomalies)\")\n","print(f\"Cross-validation set size: {X_cross_val.shape[0]} ({cv_normal_size} normal, {cv_anomaly_size} anomalies)\")\n","print(f\"Test set size: {X_test.shape[0]} ({len(X_test_normal)} normal, {len(X_test_anomaly)} anomalies)\")\n"],"metadata":{"id":"x2YPrVpFUTmF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from xgboost import XGBClassifier\n","# Modello XGBoost\n","xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n","\n","# Addestramento\n","xgb_model.fit(X_train, y_train)\n","\n","# Predizione\n","y_pred_xgb = xgb_model.predict(X_test)\n","\n","# Valutazione\n","xgboost_metrics = evaluate_model(y_test, y_pred_xgb, None, \"XG_boost\",results_df, save_results=True)\n","\n","print(y_pred_xgb)"],"metadata":{"id":"7yTS-hW2ZVb0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Best model selection\n"],"metadata":{"id":"ijvm3AVSK4c0"}},{"cell_type":"markdown","source":["Let's do a final comparison of the anomaly detection models we've trained and evaluated, ordered by F1 score."],"metadata":{"id":"0X94n5GJlYoE"}},{"cell_type":"code","source":["from IPython.display import display\n","\n","# Sort by F1 Score in descending order\n","sorted_results = results_df.sort_values(by='F1 Score', ascending=False).reset_index(drop=True)\n","\n","# Round values for cleaner appearance\n","sorted_results[['Precision', 'Recall', 'F1 Score']] = sorted_results[['Precision', 'Recall', 'F1 Score']].round(4)\n","\n","# Display the sorted table nicely\n","display(sorted_results)"],"metadata":{"id":"EHLJDGWYjdXK"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}